# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRvKzTfRq68PQluQvIWuytTvrbxI9vea
"""

import sklearn
from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import export_text
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn import tree
#1. Data Engineering
#Q.A,B
#importation des donnees
water= pd.read_csv("water_potability.csv")
print("Columns in water DataFrame:", water.columns)
# Create a copy of the DataFrame
df = water.copy()
# Display the first few rows of the DataFrame
print("DataFrame `df`:")
print(df.head())

#Q.C,description
# Display information about the dataset (columns, non-null values, data types)
print("\nDataset information:")
print(water.info())
# Summary statistics of numerical columns
print("\nSummary statistics:")
print(water.describe())
print("\nDataset shape:")
print(water.shape)
print("\nDataset size:")
print(water.size)
# Extract the target variable
target = df['Potability']
print("Target variable:")
print(target.head())
# Display the column names (features)
print("\nColumn names (features):")
print(water.columns)
# Assuming 'Potability' is the target variable
target_variable = 'Potability'
# Display unique values and value counts of the target variable
print("\nUnique values of the target variable:")
print(water[target_variable].unique())
# Value counts of the target variable
print("\nValue counts of the target variable:")
print(water[target_variable].value_counts())

#Q.D
# Check for missing values
print("Missing values per column:")
print(water.isnull().sum().sort_values(ascending=False))
# Filter numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
# Set the figure size
plt.figure(figsize=(12, 10))
# Create boxplots for each numerical column
for i, col in enumerate(numerical_columns):
    plt.subplot(4, 4, i + 1)  # Adjusting subplot layout
    sns.boxplot(x=df[col])
    plt.title(col)
plt.tight_layout()  # Adjusting layout spacing
plt.show()
# Calculate conditional means for filling missing 'ph' values
condition_1_mean_ph = df[(df['Potability'] == 0) & (df['Hardness'] <= 150)]['ph'].mean()
condition_2_mean_ph = df[(df['Potability'] == 0) & (df['Hardness'] > 150)]['ph'].mean()
condition_3_mean_ph = df[(df['Potability'] == 1) & (df['Hardness'] <= 150)]['ph'].mean()
condition_4_mean_ph = df[(df['Potability'] == 1) & (df['Hardness'] > 150)]['ph'].mean()
# Print conditional means (for verification)
print("Mean pH Value for Potability=0 and Hardness<=150:", condition_1_mean_ph)
print("Mean pH Value for Potability=0 and Hardness>150:", condition_2_mean_ph)
print("Mean pH Value for Potability=1 and Hardness<=150:", condition_3_mean_ph)
print("Mean pH Value for Potability=1 and Hardness>150:", condition_4_mean_ph)
# Iterate through the DataFrame to fill missing 'ph' values based on conditions
for idx, row in df.iterrows():
    if pd.isnull(row['ph']):  # Check if 'ph' value is missing
        if row['Potability'] == 0 and row['Hardness'] <= 150:
            df.at[idx, 'ph'] = condition_1_mean_ph
        elif row['Potability'] == 0 and row['Hardness'] > 150:
            df.at[idx, 'ph'] = condition_2_mean_ph
        elif row['Potability'] == 1 and row['Hardness'] <= 150:
            df.at[idx, 'ph'] = condition_3_mean_ph
        elif row['Potability'] == 1 and row['Hardness'] > 150:
            df.at[idx, 'ph'] = condition_4_mean_ph
# Calculate conditional mean Sulfate values
condition_1_mean_sulfate = df[df['Potability'] == 0]['Sulfate'].mean()
condition_2_mean_sulfate = df[df['Potability'] == 1]['Sulfate'].mean()
# Print conditional means (for verification)
print("Mean Sulfate Value for Potability=0:", condition_1_mean_sulfate)
print("Mean Sulfate Value for Potability=1:", condition_2_mean_sulfate)
# Iterate through the DataFrame to fill missing 'Sulfate' values
for idx, row in df.iterrows():
    if pd.isnull(row['Sulfate']):  # Check if 'Sulfate' value is missing
        if row['Potability'] == 0:
            df.at[idx, 'Sulfate'] = condition_1_mean_sulfate
        elif row['Potability'] == 1:
            df.at[idx, 'Sulfate'] = condition_2_mean_sulfate
#Trihalomethanes
df['Trihalomethanes'].fillna(value = df['Trihalomethanes'].mean() , inplace = True)
# Check for missing values after filling
print("Missing values per column:")
print(df.isnull().sum().sort_values(ascending=False))
# Check for duplicated rows
duplicates = df[df.duplicated()]
# Display duplicated rows (if any)
if not duplicates.empty:
    print("Duplicate Rows:")
    print(duplicates)
else:
    print("No duplicate rows found.")
# Calculate z-scores for each numeric column
z_scores = np.abs((df - df.mean()) / df.std())
# Define threshold for outlier detection (e.g., z-score > 3)
outlier_threshold = 3
outliers = df[z_scores > outlier_threshold]
# Visualize outliers or inspect using descriptive statistics
print("Outliers:")
print(outliers)
# Remove outliers based on z-scores
cleaned_df = df[(z_scores <= outlier_threshold).all(axis=1)]
# Set the figure size
plt.figure(figsize=(12, 10))
# Create boxplots for each numerical column
for i, col in enumerate(numerical_columns):
    plt.subplot(4, 4, i + 1)  # Adjust subplot layout as needed
    sns.boxplot(x=df[col])
    plt.title(col)
plt.tight_layout()  # Adjust layout spacing
plt.show()

#Q.E
#Analyzing, characterizing, and summarizing the cleaned dataset, using tables and plots
#description
# Display information about the dataset (columns, non-null values, data types)
print("\nDataset information:")
print(water.info())
# Summary statistics of numerical columns
print("\nSummary statistics:")
print(water.describe())
print("\nDataset shape:")
print(water.shape)
print("\nDataset size:")
print(water.size)
# Plot histograms for numerical columns
plt.figure(figsize=(12, 8))
for column in cleaned_df.select_dtypes(include='number'):
    sns.histplot(cleaned_df[column], kde=True, bins=20, alpha=0.5, label=column)
plt.legend()
plt.title("Distribution of Numerical Variables")
plt.show()
#correlation
plt.figure(figsize=(16,14))
for i,col in enumerate(df.columns):
    plt.subplot(4,3,i+1)
    sns.kdeplot(data=df[col])
    plt.tight_layout()
#sns.pairplot(df,hue='Potability')
sns.set(rc={'figure.figsize':(60,50)})
plt.figure(figsize=(10,10))
Heated = sns.heatmap(df.corr("pearson"),vmin=-1, vmax=1,cmap='viridis',annot=True, square=True)
Heated.set(xlabel = "Pearson Correlation Coefficient Heatmap")
#box plots
fig, ax = plt.subplots(10, 1, figsize=(10, 20))
fig.subplots_adjust(hspace=0.75)
for i in range(10) :
    # Ax
    sns.boxplot(x=df.columns[i], data=df, ax=ax[i])
#Potability
tar = df['Potability'].value_counts()
print(tar)
plt.figure(figsize=(8,8))
plt.pie(tar, labels=[0, 1], explode=[0, 0.01], autopct='%.f%%', shadow=True)
plt.legend()
plt.show()
# Plot histograms for numeric columns
plt.figure(figsize=(12, 8))
for col in cleaned_df.columns:
    sns.histplot(cleaned_df[col], kde=True, bins=20, alpha=0.5)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()
# Plot correlation matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()
# Plot scatter plot for two numeric variables
plt.figure(figsize=(10, 8))
sns.scatterplot(x='ph', y='Hardness', data=cleaned_df, alpha=0.5)
plt.title('Scatter Plot: pH vs Hardness')
plt.xlabel('pH')
plt.ylabel('Hardness')
plt.show()

#Q.F in rapport

#D.G
"""
# 1. Feature Scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.drop('Potability', axis=1))
"""
# 2. Dimensionality Reduction (PCA)
pca = PCA(n_components=2)  # Reduce to 2 principal components for visualization
pca_result = pca.fit_transform(scaled_features)

# Plot PCA results
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df['Potability'], cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Visualization')
plt.colorbar(label='Potability')
plt.show()

#2 model Engineering
#Q.A
#Split dataset into training set and test set
print("#Splitting Data")
x = df.iloc[:,:9]
y = df["Potability"]
print(x.shape,y.shape)
# Split the data into training and test sets ( 70% train, 30% test)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
print("Training set shapes:")
print(x_train.shape, y_train.shape)
print("Test set shapes:")
print(x_test.shape, y_test.shape)
print("Data types of training and validation sets:")
print(type(x_train), type(y_train))
print(type(x_test), type(y_test))

#Q.B
#Building Decision Tree Model
#Create Decision Tree Classifier object
treeClassifier = DecisionTreeClassifier()
#Train Decision Tree Classifier
dtree = treeClassifier.fit(x_train,y_train)
print("type(dtree)")
print(type(dtree))
print("dir(dtree)")
print(dir(dtree))


#Q.C
# Get the depth of the tree
tree_depth = treeClassifier.get_depth()
print("Depth of the tree:", tree_depth)
# Get the number of samples for each node
node_samples = treeClassifier.tree_.n_node_samples
print("Number of samples in each node:", node_samples)
# Get the total number of nodes in the tree
total_nodes = treeClassifier.tree_.node_count
print("Total number of nodes in the tree:", total_nodes)
print("dir(treeClassifier)")
print(dir(treeClassifier))
print("n_leaves:",treeClassifier.get_n_leaves())
print("tree depth:",treeClassifier.get_depth())
print("features_names_in_:",treeClassifier.feature_names_in_)
print("max_features_:",treeClassifier.max_features_)
print("min_impurity_decrease:",treeClassifier.min_impurity_decrease)
print("min_samples_leaf:",treeClassifier.min_samples_leaf)
print("n_classes:",treeClassifier.n_classes_)
print("n_feautures_in_:",treeClassifier.n_features_in_)
print("max_features:",treeClassifier.max_features)

print("get_params:",treeClassifier.get_params())
print("set_params:",treeClassifier.set_params())
print("n_outputs_:",treeClassifier.n_outputs_)
print("max_depth:",treeClassifier.max_depth)
print("classes_:",treeClassifier.classes_)
print("decision_path:",treeClassifier.decision_path)
#print("_support_missing_values:",treeClassifier._support_missing_values)
print("_getattribute_:",treeClassifier.__getattribute__)
print("tree_text")
print(export_text(treeClassifier))
print(treeClassifier.tree_.children_left)
print(treeClassifier.tree_.children_right)
#print(treeClassifier.tree_.compute_node_depths())
print(dir(treeClassifier))

print("Number of leaves:", treeClassifier.get_n_leaves())
print("Tree depth:", treeClassifier.get_depth())

print("Feature names:", treeClassifier.feature_names_in_)
print("Max features:", treeClassifier.max_features_)

print("Min impurity decrease:", treeClassifier.min_impurity_decrease)
print("Min samples leaf:", treeClassifier.min_samples_leaf)

print("Min samples split:", treeClassifier.min_samples_split)

print("Number of classes:", treeClassifier.n_classes_)
print("Number of features in:", treeClassifier.n_features_in_)

print("Max features:", treeClassifier.max_features)

#print("Support missing values:", treeClassifier._support_missing_values)

print("Get params:", treeClassifier.get_params())

print("Set params:", treeClassifier.set_params())
print("Number of outputs:", treeClassifier.n_outputs_)

print("Max depth:", treeClassifier.max_depth)

print("Classes:", treeClassifier.classes_)

print("Decision path:", treeClassifier.decision_path)

print("_getattr_:", treeClassifier.__getattribute__)

from sklearn.tree import plot_tree
plt.figure(figsize=(200,200))

plot_tree(treeClassifier,filled=True)
plt.show()
print(export_text(treeClassifier))


from sklearn.tree import plot_tree
# Plot the entire tree
plt.figure(figsize=(20, 20))
plot_tree(treeClassifier, filled=True, fontsize=10)
plt.show()

#Q2D
'''
# Get feature importances
feature_importances = clf.feature_importances_

# Create a DataFrame to display feature importances
importance_df = pd.DataFrame({
    'Feature': df.columns,
    'Importance': feature_importances
})

# Sort features by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

# Most relevant features
top_features = importance_df.head()

print("Top 5 Relevant Features:")
print(top_features)
'''
#Q.E
# Extract specific parts of the tree for nodes with 3 to 5 samples
def extract_subtree(tree, node_id, sample_threshold):
    #Recursively extract the subtree starting from the given node.
    if tree.children_left[node_id] == tree.children_right[node_id]:  # Leaf node
        return [(node_id, tree.value[node_id])]

    # Internal node
    left_subtree = extract_subtree(tree, tree.children_left[node_id], sample_threshold)
    right_subtree = extract_subtree(tree, tree.children_right[node_id], sample_threshold)

    if sum([item[1].sum() for item in left_subtree]) >= sample_threshold:
        return left_subtree
    elif sum([item[1].sum() for item in right_subtree]) >= sample_threshold:
        return right_subtree
    else:
        return [(node_id, tree.value[node_id])] + left_subtree + right_subtree

# Find nodes with 3 to 5 samples and extract the subtrees
nodes_of_interest = []
for node_id in range(treeClassifier.tree_.node_count):
    if (treeClassifier.tree_.children_left[node_id] != treeClassifier.tree_.children_right[node_id] and
        treeClassifier.tree_.n_node_samples[node_id] >= 3 and treeClassifier.tree_.n_node_samples[node_id] <= 5):
        nodes_of_interest.append(node_id)

for node_id in nodes_of_interest:
    subtree = extract_subtree(treeClassifier.tree_, node_id, 3)
    plt.figure(figsize=(10, 6))
    plot_tree(treeClassifier, max_depth=3, node_ids=[n[0] for n in subtree], filled=True, fontsize=8)
    plt.title(f"Subtree for Node {node_id} (Samples: {treeClassifier.tree_.n_node_samples[node_id]})")
    plt.show()

#Q f
# Predict the target variable on the training set
y_train_pred = treeClassifier.predict(x_train)
# Calculate the accuracy of the model
training_accuracy = accuracy_score(y_train, y_train_pred)
print("Training Accuracy:", training_accuracy)

#Qg
# Predict the target variable on the test set
y_test_pred = treeClassifier.predict(x_test)
# Calculate the accuracy of the model
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy:", test_accuracy)

#Q h
# Define the parameter grid for grid search
param_grid = {
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5]
}
# Perform grid search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(treeClassifier, param_grid, cv=5)
grid_search.fit(x_train, y_train)
# Get the best estimator/model from the grid search
final_model = grid_search.best_estimator_
best_parameters = grid_search.best_params_
print("Best Parameters:", best_parameters)
# Train the final model with the best parameters obtained from grid search
final_model.fit(x_train, y_train)
# Evaluate the final model on the test set
test_accuracy = final_model.score(x_test, y_test)
print("Test Accuracy with Optimized Parameters:", test_accuracy)
# Print tree depth of the final model
print("Tree Depth of Final Model:", final_model.get_depth())

#Q.I
# Size of the original tree
original_tree_size = dtree.tree_.node_count
original_tree_leaves = dtree.tree_.n_leaves
# Size of the pruned tree
pruned_tree_size = final_model.tree_.node_count
pruned_tree_leaves = final_model.tree_.n_leaves
print("Original Tree Size (Nodes):", original_tree_size)
print("Original Tree Size (Leaves):", original_tree_leaves)
print("Pruned (pre pruning)")
print("Pruned Tree Size (Nodes):", pruned_tree_size)
print("Pruned Tree Size (Leaves):", pruned_tree_leaves)

#Q.J
from sklearn.metrics import accuracy_score
# Predictions on the training set
y_train_pred_unpruned = treeClassifier.predict(x_train)
y_train_pred_pruned = final_model.predict(x_train)
# Predictions on the test set
y_test_pred_unpruned = treeClassifier.predict(x_test)
y_test_pred_pruned = final_model.predict(x_test)
# Compute accuracies
accuracy_train_unpruned = accuracy_score(y_train, y_train_pred_unpruned)
accuracy_train_pruned = accuracy_score(y_train, y_train_pred_pruned)
accuracy_test_unpruned = accuracy_score(y_test, y_test_pred_unpruned)
accuracy_test_pruned = accuracy_score(y_test, y_test_pred_pruned)
print("Accuracy of unpruned tree on training set:", accuracy_train_unpruned)
print("Accuracy of pruned tree on training set:", accuracy_train_pruned)
print("*******************")
print("Accuracy of unpruned tree on test set:", accuracy_test_unpruned)
print("Accuracy of pruned tree on test set:", accuracy_test_pruned)
print("*******************")

#Q.K
# Further split the data into training, validation
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)
# Train the decision tree on the training set
tree = DecisionTreeClassifier(random_state=42)
tree.fit(x_train, y_train)
# Evaluate the initial tree on the validation set
initial_val_accuracy = accuracy_score(y_val, tree.predict(x_val))
print("Initial Validation Accuracy:", initial_val_accuracy)
# Prune the tree using Cost Complexity Pruning (CCP)
path = tree.cost_complexity_pruning_path(x_train, y_train)
ccp_alphas, impurities = path.ccp_alphas[:-1], path.impurities[:-1]
pruned_trees = []
for ccp_alpha in ccp_alphas:
    pruned_tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    pruned_tree.fit(x_train, y_train)
    pruned_trees.append(pruned_tree)
# Find the pruned tree with the best validation accuracy
best_tree = None
best_val_accuracy = initial_val_accuracy
for tree in pruned_trees:
    val_accuracy = accuracy_score(y_val, tree.predict(x_val))
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_tree = tree
# Evaluate the best pruned tree on the test set
if best_tree:
    pruned_tree_accuracy = accuracy_score(y_test, best_tree.predict(x_test))
    print("Accuracy of the pruned tree on the test set:", pruned_tree_accuracy)
else:
    print("No pruned tree selected.")

#Q.L
# Size of the original tree
original_tree_size = dtree.tree_.node_count
original_tree_leaves = dtree.tree_.n_leaves
# Size of the pruned tree
pruned_tree_size2 = best_tree.tree_.node_count
pruned_tree_leaves2 = best_tree.tree_.n_leaves
print("Pruned (post pruning)")
print("Original Tree Size (Nodes):", original_tree_size)
print("Original Tree Size (Leaves):", original_tree_leaves)
print("******")
print("Best Pruned Tree Size  (Nodes):", pruned_tree_size2)
print("Best Pruned Tree Size   (Leaves):", pruned_tree_leaves2)
print("******")

#Q.M
train_accuracy_unpruned = accuracy_score(y_train, treeClassifier.predict(x_train))
test_accuracy_unpruned = accuracy_score(y_test, treeClassifier.predict(x_test))
val_accuracy_unpruned = accuracy_score(y_val, treeClassifier.predict(x_val))
# Pruned tree accuracies
if best_tree:
    train_accuracy_pruned = accuracy_score(y_train, best_tree.predict(x_train))
    test_accuracy_pruned = accuracy_score(y_test, best_tree.predict(x_test))
    val_accuracy_pruned = accuracy_score(y_val, best_tree.predict(x_val))
else:
    train_accuracy_pruned = None
    test_accuracy_pruned = None
    val_accuracy_pruned = None

# Print the accuracies
print("Unpruned Tree:")
print("Training Accuracy:", train_accuracy_unpruned)
print("Test Accuracy:", test_accuracy_unpruned)
print("Validation Accuracy:", val_accuracy_unpruned)

if best_tree:
    print("\nPruned Tree:")
    print("Training Accuracy:", train_accuracy_pruned)
    print("Test Accuracy:", test_accuracy_pruned)
    print("Validation Accuracy:", val_accuracy_pruned)
else:
    print("\nNo pruned tree selected.")

print("***Final Question*********")

#Q.P
# Pre-pruning: Train the decision tree with pre-pruning
pre_pruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10, random_state=42)
pre_pruned_tree.fit(x_train, y_train)
# Post-pruning: Prune the tree using cost-complexity pruning (CCP) on the validation set
path = pre_pruned_tree.cost_complexity_pruning_path(x_val, y_val)
ccp_alphas, impurities = path.ccp_alphas[:-1], path.impurities[:-1]
# Evaluate the pruned trees on the validation set
pruned_trees = []
for ccp_alpha in ccp_alphas:
    pruned_tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    pruned_tree.fit(x_train, y_train)
    pruned_trees.append(pruned_tree)
# Find the pruned tree with the best validation accuracy
best_tree = None
best_val_accuracy = 0.0
for tree in pruned_trees:
    val_accuracy = accuracy_score(y_val, tree.predict(x_val))
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_tree = tree
# Evaluate the best pruned tree on the test set
if best_tree:
    test_accuracy = accuracy_score(y_test, best_tree.predict(x_test))
    print("Accuracy of the pruned tree on the test set:", test_accuracy)
else:
    print("No pruned tree selected.")

# Set the figure size
plt.figure(figsize=(12, 10))

# Create boxplots for each numerical column
for i, col in enumerate(numerical_columns):
    plt.subplot(4, 4, i + 1)  # Adjust subplot layout as needed
    sns.boxplot(x=df[col])
    plt.title(col)

plt.tight_layout()  # Adjust layout spacing
plt.show()


# Calculate conditional means for filling missing 'ph' values
condition_1_mean_ph = df[(df['Potability'] == 0) & (df['Hardness'] <= 150)]['ph'].mean()
condition_2_mean_ph = df[(df['Potability'] == 0) & (df['Hardness'] > 150)]['ph'].mean()
condition_3_mean_ph = df[(df['Potability'] == 1) & (df['Hardness'] <= 150)]['ph'].mean()
condition_4_mean_ph = df[(df['Potability'] == 1) & (df['Hardness'] > 150)]['ph'].mean()

# Print conditional means (for verification)
print("Mean pH Value for Potability=0 and Hardness<=150:", condition_1_mean_ph)
print("Mean pH Value for Potability=0 and Hardness>150:", condition_2_mean_ph)
print("Mean pH Value for Potability=1 and Hardness<=150:", condition_3_mean_ph)
print("Mean pH Value for Potability=1 and Hardness>150:", condition_4_mean_ph)

# Iterate through the DataFrame to fill missing 'ph' values based on conditions
for idx, row in df.iterrows():
    if pd.isnull(row['ph']):  # Check if 'ph' value is missing
        if row['Potability'] == 0 and row['Hardness'] <= 150:
            df.at[idx, 'ph'] = condition_1_mean_ph
        elif row['Potability'] == 0 and row['Hardness'] > 150:
            df.at[idx, 'ph'] = condition_2_mean_ph
        elif row['Potability'] == 1 and row['Hardness'] <= 150:
            df.at[idx, 'ph'] = condition_3_mean_ph
        elif row['Potability'] == 1 and row['Hardness'] > 150:
            df.at[idx, 'ph'] = condition_4_mean_ph

# Calculate conditional mean Sulfate values
condition_1_mean_sulfate = df[df['Potability'] == 0]['Sulfate'].mean()
condition_2_mean_sulfate = df[df['Potability'] == 1]['Sulfate'].mean()

# Print conditional means (for verification)
print("Mean Sulfate Value for Potability=0:", condition_1_mean_sulfate)
print("Mean Sulfate Value for Potability=1:", condition_2_mean_sulfate)
# Iterate through the DataFrame to fill missing 'Sulfate' values
for idx, row in df.iterrows():
    if pd.isnull(row['Sulfate']):  # Check if 'Sulfate' value is missing
        if row['Potability'] == 0:
            df.at[idx, 'Sulfate'] = condition_1_mean_sulfate
        elif row['Potability'] == 1:
            df.at[idx, 'Sulfate'] = condition_2_mean_sulfate
#Trihalomethanes
df['Trihalomethanes'].fillna(value = df['Trihalomethanes'].mean() , inplace = True)
# Check for missing values after filling
print("Missing values per column:")
print(df.isnull().sum().sort_values(ascending=False))

# Check for duplicated rows
duplicates = df[df.duplicated()]

# Display duplicated rows (if any)
if not duplicates.empty:
    print("Duplicate Rows:")
    print(duplicates)
else:
    print("No duplicate rows found.")

# Calculate z-scores for each numeric column
z_scores = np.abs((df - df.mean()) / df.std())

# Define threshold for outlier detection (e.g., z-score > 3)
outlier_threshold = 3
outliers = df[z_scores > outlier_threshold]

# Visualize outliers or inspect using descriptive statistics
print("Outliers:")
print(outliers)
# Remove outliers based on z-scores
cleaned_df = df[(z_scores <= outlier_threshold).all(axis=1)]

# Set the figure size
plt.figure(figsize=(12, 10))

# Create boxplots for each numerical column
for i, col in enumerate(numerical_columns):
    plt.subplot(4, 4, i + 1)  # Adjust subplot layout as needed
    sns.boxplot(x=df[col])
    plt.title(col)

plt.tight_layout()  # Adjust layout spacing
plt.show()
#Q.E
#Analyzing, characterizing, and summarizing the cleaned dataset, using tables and plots
#description
# Display information about the dataset (columns, non-null values, data types)
print("\nDataset information:")
print(water.info())

# Summary statistics of numerical columns
print("\nSummary statistics:")
print(water.describe())

print("\nDataset shape:")
print(water.shape)

print("\nDataset size:")
print(water.size)
'''
# Plot histograms for numerical columns
plt.figure(figsize=(12, 8))
for column in cleaned_df.select_dtypes(include='number'):
    sns.histplot(cleaned_df[column], kde=True, bins=20, alpha=0.5, label=column)
plt.legend()
plt.title("Distribution of Numerical Variables")
plt.show()
#correlation
plt.figure(figsize=(16,14))
for i,col in enumerate(df.columns):
    plt.subplot(4,3,i+1)
    sns.kdeplot(data=df[col])
    plt.tight_layout()



#sns.pairplot(df,hue='Potability')
sns.set(rc={'figure.figsize':(60,50)})
plt.figure(figsize=(10,10))
Heated = sns.heatmap(df.corr("pearson"),vmin=-1, vmax=1,cmap='viridis',annot=True, square=True)
Heated.set(xlabel = "Pearson Correlation Coefficient Heatmap")

#box plots
fig, ax = plt.subplots(10, 1, figsize=(10, 20))
fig.subplots_adjust(hspace=0.75)
for i in range(10) :
    # Ax
    sns.boxplot(x=df.columns[i], data=df, ax=ax[i])

#Potability
tar = df['Potability'].value_counts()
print(tar)
plt.figure(figsize=(8,8))
plt.pie(tar, labels=[0, 1], explode=[0, 0.01], autopct='%.f%%', shadow=True)
plt.legend()
plt.show()



# Plot histograms for numeric columns
plt.figure(figsize=(12, 8))
for col in cleaned_df.columns:
    sns.histplot(cleaned_df[col], kde=True, bins=20, alpha=0.5)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()
# Plot correlation matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()
# Plot scatter plot for two numeric variables
plt.figure(figsize=(10, 8))
sns.scatterplot(x='ph', y='Hardness', data=cleaned_df, alpha=0.5)
plt.title('Scatter Plot: pH vs Hardness')
plt.xlabel('pH')
plt.ylabel('Hardness')
plt.show()
'''
#Q.F talk about the dataset before cleaning(before Q.E)and after
'''


# 1. Feature Scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.drop('Potability', axis=1))

# 2. Dimensionality Reduction (PCA)
pca = PCA(n_components=2)  # Reduce to 2 principal components for visualization
pca_result = pca.fit_transform(scaled_features)

# Plot PCA results
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df['Potability'], cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Visualization')
plt.colorbar(label='Potability')
plt.show()
'''


#2 model Engineering

#Splitting Data
#Split dataset into training set and test set
print("#Splitting Data")
x = df.iloc[:,:9]
y = df["Potability"]
print(x.shape,y.shape)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
print("Training set shapes:")
print(x_train.shape, y_train.shape)
print("Test set shapes:")
print(x_test.shape, y_test.shape)
print("Data types of training and validation sets:")
print(type(x_train), type(y_train))
print(type(x_test), type(y_test))

#Q.B
#Building Decision Tree Model

#Create Decision Tree Classifier object
treeClassifier = DecisionTreeClassifier()

#Train Decision Tree Classifier
dtree = treeClassifier.fit(x_train,y_train)
print("type(dtree)")
print(type(dtree))
print("dir(dtree)")
print(dir(dtree))


#Q.C
# Get the depth of the tree
tree_depth = treeClassifier.get_depth()
print("Depth of the tree:", tree_depth)
# Get the number of samples for each node
node_samples = treeClassifier.tree_.n_node_samples
print("Number of samples in each node:", node_samples)
# Get the total number of nodes in the tree
total_nodes = treeClassifier.tree_.node_count
print("Total number of nodes in the tree:", total_nodes)


'''
print("dir(treeClassifier)")
print(dir(treeClassifier))
print("n_leaves:",treeClassifier.get_n_leaves())
print("tree depth:",treeClassifier.get_depth())
print("features_names_in_:",treeClassifier.feature_names_in_)
print("max_features_:",treeClassifier.max_features_)
print("min_impurity_decrease:",treeClassifier.min_impurity_decrease)
print("min_samples_leaf:",treeClassifier.min_samples_leaf)
print("n_classes:",treeClassifier.n_classes_)
print("n_feautures_in_:",treeClassifier.n_features_in_)
print("max_features:",treeClassifier.max_features)

print("get_params:",treeClassifier.get_params())
print("set_params:",treeClassifier.set_params())
print("n_outputs_:",treeClassifier.n_outputs_)
print("max_depth:",treeClassifier.max_depth)
print("classes_:",treeClassifier.classes_)
print("decision_path:",treeClassifier.decision_path)
#print("_support_missing_values:",treeClassifier._support_missing_values)
print("_getattribute_:",treeClassifier.__getattribute__)


print("tree_text")

print(export_text(treeClassifier))
print(treeClassifier.tree_.children_left)
print(treeClassifier.tree_.children_right)
#print(treeClassifier.tree_.compute_node_depths())


print(dir(treeClassifier))

print("Number of leaves:", treeClassifier.get_n_leaves())
print("Tree depth:", treeClassifier.get_depth())

print("Feature names:", treeClassifier.feature_names_in_)
print("Max features:", treeClassifier.max_features_)

print("Min impurity decrease:", treeClassifier.min_impurity_decrease)
print("Min samples leaf:", treeClassifier.min_samples_leaf)

print("Min samples split:", treeClassifier.min_samples_split)

print("Number of classes:", treeClassifier.n_classes_)
print("Number of features in:", treeClassifier.n_features_in_)

print("Max features:", treeClassifier.max_features)

#print("Support missing values:", treeClassifier._support_missing_values)

print("Get params:", treeClassifier.get_params())

print("Set params:", treeClassifier.set_params())
print("Number of outputs:", treeClassifier.n_outputs_)

print("Max depth:", treeClassifier.max_depth)

print("Classes:", treeClassifier.classes_)

print("Decision path:", treeClassifier.decision_path)

print("_getattr_:", treeClassifier.__getattribute__)

from sklearn.tree import plot_tree
plt.figure(figsize=(200,200))

plot_tree(treeClassifier,filled=True)
plt.show()
print(export_text(treeClassifier))  """
'''
#Q2D
from sklearn.tree import plot_tree

'''
# Plot the entire tree
plt.figure(figsize=(20, 20))
plot_tree(treeClassifier, filled=True, fontsize=10)
plt.show()

# Extract specific parts of the tree for nodes with 3 to 5 samples
def extract_subtree(tree, node_id, sample_threshold):
    """Recursively extract the subtree starting from the given node."""
    if tree.children_left[node_id] == tree.children_right[node_id]:  # Leaf node
        return [(node_id, tree.value[node_id])]

    # Internal node
    left_subtree = extract_subtree(tree, tree.children_left[node_id], sample_threshold)
    right_subtree = extract_subtree(tree, tree.children_right[node_id], sample_threshold)

    if sum([item[1].sum() for item in left_subtree]) >= sample_threshold:
        return left_subtree
    elif sum([item[1].sum() for item in right_subtree]) >= sample_threshold:
        return right_subtree
    else:
        return [(node_id, tree.value[node_id])] + left_subtree + right_subtree

# Find nodes with 3 to 5 samples and extract the subtrees
nodes_of_interest = []
for node_id in range(treeClassifier.tree_.node_count):
    if (treeClassifier.tree_.children_left[node_id] != treeClassifier.tree_.children_right[node_id] and
        treeClassifier.tree_.n_node_samples[node_id] >= 3 and treeClassifier.tree_.n_node_samples[node_id] <= 5):
        nodes_of_interest.append(node_id)

for node_id in nodes_of_interest:
    subtree = extract_subtree(treeClassifier.tree_, node_id, 3)  # Adjust sample threshold as needed
    plt.figure(figsize=(10, 6))
    plot_tree(treeClassifier, max_depth=3, node_ids=[n[0] for n in subtree], filled=True, fontsize=8)
    plt.title(f"Subtree for Node {node_id} (Samples: {treeClassifier.tree_.n_node_samples[node_id]})")
    plt.show()
    '''
#Q f

# Predict the target variable on the training set
y_train_pred = treeClassifier.predict(x_train)

# Calculate the accuracy of the model
training_accuracy = accuracy_score(y_train, y_train_pred)

print("Training Accuracy:", training_accuracy)


#Qg

# Predict the target variable on the test set
y_test_pred = treeClassifier.predict(x_test)

# Calculate the accuracy of the model
test_accuracy = accuracy_score(y_test, y_test_pred)

print("Test Accuracy:", test_accuracy)
#Q h


# Define the parameter grid for grid search
param_grid = {
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5]
}



# Perform grid search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(treeClassifier, param_grid, cv=5)
grid_search.fit(x_train, y_train)

# Get the best estimator/model from the grid search
final_model = grid_search.best_estimator_
best_parameters = grid_search.best_params_
print("Best Parameters:", best_parameters)
# Train the final model with the best parameters obtained from grid search
final_model.fit(x_train, y_train)

# Evaluate the final model on the test set
test_accuracy = final_model.score(x_test, y_test)
print("Test Accuracy with Optimized Parameters:", test_accuracy)

# Print tree depth of the final model
print("Tree Depth of Final Model:", final_model.get_depth())



#Q.I
# Size of the original tree
original_tree_size = dtree.tree_.node_count
original_tree_leaves = dtree.tree_.n_leaves

# Size of the pruned tree
pruned_tree_size = final_model.tree_.node_count
pruned_tree_leaves = final_model.tree_.n_leaves

print("Original Tree Size (Nodes):", original_tree_size)
print("Original Tree Size (Leaves):", original_tree_leaves)
print("Pruned (pre pruninh)")
print("Pruned Tree Size (Nodes):", pruned_tree_size)
print("Pruned Tree Size (Leaves):", pruned_tree_leaves)
#Q.J
from sklearn.metrics import accuracy_score

# Predictions on the training set
y_train_pred_unpruned = treeClassifier.predict(x_train)
y_train_pred_pruned = final_model.predict(x_train)

# Predictions on the test set
y_test_pred_unpruned = treeClassifier.predict(x_test)
y_test_pred_pruned = final_model.predict(x_test)

# Compute accuracies
accuracy_train_unpruned = accuracy_score(y_train, y_train_pred_unpruned)
accuracy_train_pruned = accuracy_score(y_train, y_train_pred_pruned)

accuracy_test_unpruned = accuracy_score(y_test, y_test_pred_unpruned)
accuracy_test_pruned = accuracy_score(y_test, y_test_pred_pruned)



print("Accuracy of unpruned tree on training set:", accuracy_train_unpruned)
print("Accuracy of pruned tree on training set:", accuracy_train_pruned)
print("*******************")

print("Accuracy of unpruned tree on test set:", accuracy_test_unpruned)
print("Accuracy of pruned tree on test set:", accuracy_test_pruned)
print("*******************")


#Q.K


# Further split the data into training, validation
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Train the decision tree on the training set
tree = DecisionTreeClassifier(random_state=42)
tree.fit(x_train, y_train)

# Evaluate the initial tree on the validation set
initial_val_accuracy = accuracy_score(y_val, tree.predict(x_val))
print("Initial Validation Accuracy:", initial_val_accuracy)

# Prune the tree using Cost Complexity Pruning (CCP)
path = tree.cost_complexity_pruning_path(x_train, y_train)
ccp_alphas, impurities = path.ccp_alphas[:-1], path.impurities[:-1]

pruned_trees = []
for ccp_alpha in ccp_alphas:
    pruned_tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    pruned_tree.fit(x_train, y_train)
    pruned_trees.append(pruned_tree)

# Find the pruned tree with the best validation accuracy
best_tree = None
best_val_accuracy = initial_val_accuracy

for tree in pruned_trees:
    val_accuracy = accuracy_score(y_val, tree.predict(x_val))
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_tree = tree

# Evaluate the best pruned tree on the test set
if best_tree:
    pruned_tree_accuracy = accuracy_score(y_test, best_tree.predict(x_test))
    print("Accuracy of the pruned tree on the test set:", pruned_tree_accuracy)
else:
    print("No pruned tree selected.")
#Q.L
# Size of the original tree
original_tree_size = dtree.tree_.node_count
original_tree_leaves = dtree.tree_.n_leaves

# Size of the pruned tree
pruned_tree_size2 = best_tree.tree_.node_count
pruned_tree_leaves2 = best_tree.tree_.n_leaves

print("Pruned (post pruninh)")
print("Original Tree Size (Nodes):", original_tree_size)
print("Original Tree Size (Leaves):", original_tree_leaves)

print("******")
print("Best Pruned Tree Size  (Nodes):", pruned_tree_size2)
print("Best Pruned Tree Size   (Leaves):", pruned_tree_leaves2)
print("******")

#Q.M




# Assuming 'treeClassifier' is the unpruned decision tree model you previously trained
train_accuracy_unpruned = accuracy_score(y_train, treeClassifier.predict(x_train))
test_accuracy_unpruned = accuracy_score(y_test, treeClassifier.predict(x_test))
val_accuracy_unpruned = accuracy_score(y_val, treeClassifier.predict(x_val))

# Pruned tree accuracies
if best_tree:
    train_accuracy_pruned = accuracy_score(y_train, best_tree.predict(x_train))
    test_accuracy_pruned = accuracy_score(y_test, best_tree.predict(x_test))
    val_accuracy_pruned = accuracy_score(y_val, best_tree.predict(x_val))
else:
    train_accuracy_pruned = None
    test_accuracy_pruned = None
    val_accuracy_pruned = None

# Print the accuracies
print("Unpruned Tree:")
print("Training Accuracy:", train_accuracy_unpruned)
print("Test Accuracy:", test_accuracy_unpruned)
print("Validation Accuracy:", val_accuracy_unpruned)

if best_tree:
    print("\nPruned Tree:")
    print("Training Accuracy:", train_accuracy_pruned)
    print("Test Accuracy:", test_accuracy_pruned)
    print("Validation Accuracy:", val_accuracy_pruned)
else:
    print("\nNo pruned tree selected.")

#Q.P
print("***Final Question*********")
# Pre-pruning: Train the decision tree with pre-pruning
pre_pruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10, random_state=42)
pre_pruned_tree.fit(x_train, y_train)

# Post-pruning: Prune the tree using cost-complexity pruning (CCP) on the validation set
path = pre_pruned_tree.cost_complexity_pruning_path(x_val, y_val)
ccp_alphas, impurities = path.ccp_alphas[:-1], path.impurities[:-1]

# Evaluate the pruned trees on the validation set
pruned_trees = []
for ccp_alpha in ccp_alphas:
    pruned_tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    pruned_tree.fit(x_train, y_train)
    pruned_trees.append(pruned_tree)

# Find the pruned tree with the best validation accuracy
best_tree = None
best_val_accuracy = 0.0
for tree in pruned_trees:
    val_accuracy = accuracy_score(y_val, tree.predict(x_val))
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_tree = tree

# Evaluate the best pruned tree on the test set
if best_tree:
    test_accuracy = accuracy_score(y_test, best_tree.predict(x_test))
    print("Accuracy of the pruned tree on the test set:", test_accuracy)
else:
    print("No pruned tree selected.")

import sklearn
from sklearn import datasets
import pandas as pd
import numpy as np
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
#importation des donnees
water= pd.read_csv("water_potability.csv")
print("Columns in water DataFrame:", water.columns)
df = water.copy()  # Create a copy of the DataFrame
print("DataFrame `df`:")

z_scores = np.abs(stats.zscore(df['numeric_column']))
threshold = 3
outlier_indices = np.where(z_scores > threshold)[0]

cleaned_df = df.drop(outlier_indices)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv('water_potability.csv')

# Basic data exploration
print(data.info())
print(data.describe())

# Check for missing values
print(data.isnull().sum())

# Drop duplicates
data.drop_duplicates(inplace=True)

# Handle missing values (e.g., using mean/median imputation)
data.fillna(data.mean(), inplace=True)

# Data visualization
#sns.pairplot(data, hue='Potability')
#plt.show()

# Correlation heatmap
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

# Preprocess data for ML
X = data.drop('Potability', axis=1)
y = data['Potability']

# Further analysis - e.g., feature engineering, PCA, etc.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
data = pd.read_csv('water_potability.csv')

# Handling missing values by filling with mean
data.fillna(data.mean(), inplace=True)

# Separate features (X) and target variable (y)
X = data.drop('Potability', axis=1)
y = data['Potability']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Engineering: Polynomial Features + Scaling
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

scaler = StandardScaler()
X_train_poly_scaled = scaler.fit_transform(X_train_poly)
X_test_poly_scaled = scaler.transform(X_test_poly)

# Training a Random Forest Classifier on the engineered features
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_poly_scaled, y_train)

# Predictions on the test set
y_pred = rf_classifier.predict(X_test_poly_scaled)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
data = pd.read_csv('water_potability.csv')

# Handling missing values by filling with mean
data.fillna(data.mean(), inplace=True)

# Separate features (X) and target variable (y)
X = data.drop('Potability', axis=1)
y = data['Potability']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Applying PCA for dimensionality reduction
pca = PCA(n_components=8)  # Selecting 8 principal components
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Training a Random Forest Classifier on the PCA-transformed features
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_pca, y_train)

# Predictions on the test set
y_pred = rf_classifier.predict(X_test_pca)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Explained variance ratio of the selected principal components
print("\nExplained Variance Ratio:")
print(pca.explained_variance_ratio_)